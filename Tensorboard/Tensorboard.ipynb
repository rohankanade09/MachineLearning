{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping,TensorBoard\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cancer_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('benign_0__mal_1',axis=1).values\n",
    "y = df['benign_0__mal_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rohan/Documents/JupyterProjects/Practice'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = '/TensorboardLogs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = TensorBoard(log_dir = logs,\n",
    "                    histogram_freq = 1,\n",
    "                    write_graph=True,\n",
    "                    write_images=False,\n",
    "                    update_freq='epoch',\n",
    "                    profile_batch=2,\n",
    "                    embeddings_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=30,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=15,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/600\n",
      "426/426 [==============================] - 3s 6ms/sample - loss: 0.7106 - val_loss: 0.6682\n",
      "Epoch 2/600\n",
      "426/426 [==============================] - 0s 325us/sample - loss: 0.6915 - val_loss: 0.6610\n",
      "Epoch 3/600\n",
      "426/426 [==============================] - 0s 270us/sample - loss: 0.6670 - val_loss: 0.6502\n",
      "Epoch 4/600\n",
      "426/426 [==============================] - 0s 243us/sample - loss: 0.6563 - val_loss: 0.6339\n",
      "Epoch 5/600\n",
      "426/426 [==============================] - 0s 185us/sample - loss: 0.6342 - val_loss: 0.6006\n",
      "Epoch 6/600\n",
      "426/426 [==============================] - 0s 201us/sample - loss: 0.6182 - val_loss: 0.5789\n",
      "Epoch 7/600\n",
      "426/426 [==============================] - 0s 195us/sample - loss: 0.6134 - val_loss: 0.5576\n",
      "Epoch 8/600\n",
      "426/426 [==============================] - 0s 199us/sample - loss: 0.6044 - val_loss: 0.5356\n",
      "Epoch 9/600\n",
      "426/426 [==============================] - 0s 189us/sample - loss: 0.5636 - val_loss: 0.5084\n",
      "Epoch 10/600\n",
      "426/426 [==============================] - 0s 355us/sample - loss: 0.5391 - val_loss: 0.4763\n",
      "Epoch 11/600\n",
      "426/426 [==============================] - 0s 204us/sample - loss: 0.5234 - val_loss: 0.4495\n",
      "Epoch 12/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.4666 - val_loss: 0.4222\n",
      "Epoch 13/600\n",
      "426/426 [==============================] - 0s 283us/sample - loss: 0.4919 - val_loss: 0.3952\n",
      "Epoch 14/600\n",
      "426/426 [==============================] - 0s 208us/sample - loss: 0.4576 - val_loss: 0.3707\n",
      "Epoch 15/600\n",
      "426/426 [==============================] - 0s 158us/sample - loss: 0.4345 - val_loss: 0.3519\n",
      "Epoch 16/600\n",
      "426/426 [==============================] - 0s 175us/sample - loss: 0.4315 - val_loss: 0.3348\n",
      "Epoch 17/600\n",
      "426/426 [==============================] - 0s 165us/sample - loss: 0.4077 - val_loss: 0.3202\n",
      "Epoch 18/600\n",
      "426/426 [==============================] - 0s 161us/sample - loss: 0.3996 - val_loss: 0.3013\n",
      "Epoch 19/600\n",
      "426/426 [==============================] - 0s 156us/sample - loss: 0.4060 - val_loss: 0.2846\n",
      "Epoch 20/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.3432 - val_loss: 0.2744\n",
      "Epoch 21/600\n",
      "426/426 [==============================] - 0s 175us/sample - loss: 0.3515 - val_loss: 0.2535\n",
      "Epoch 22/600\n",
      "426/426 [==============================] - 0s 158us/sample - loss: 0.3727 - val_loss: 0.2410\n",
      "Epoch 23/600\n",
      "426/426 [==============================] - 0s 175us/sample - loss: 0.3263 - val_loss: 0.2325\n",
      "Epoch 24/600\n",
      "426/426 [==============================] - 0s 203us/sample - loss: 0.3210 - val_loss: 0.2291\n",
      "Epoch 25/600\n",
      "426/426 [==============================] - 0s 187us/sample - loss: 0.2932 - val_loss: 0.2120\n",
      "Epoch 26/600\n",
      "426/426 [==============================] - 0s 184us/sample - loss: 0.3129 - val_loss: 0.2021\n",
      "Epoch 27/600\n",
      "426/426 [==============================] - 0s 171us/sample - loss: 0.3274 - val_loss: 0.1985\n",
      "Epoch 28/600\n",
      "426/426 [==============================] - 0s 172us/sample - loss: 0.3072 - val_loss: 0.1932\n",
      "Epoch 29/600\n",
      "426/426 [==============================] - 0s 422us/sample - loss: 0.2985 - val_loss: 0.1876\n",
      "Epoch 30/600\n",
      "426/426 [==============================] - 0s 216us/sample - loss: 0.2860 - val_loss: 0.1771\n",
      "Epoch 31/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.2993 - val_loss: 0.1729\n",
      "Epoch 32/600\n",
      "426/426 [==============================] - 0s 165us/sample - loss: 0.2505 - val_loss: 0.1732\n",
      "Epoch 33/600\n",
      "426/426 [==============================] - 0s 154us/sample - loss: 0.2653 - val_loss: 0.1674\n",
      "Epoch 34/600\n",
      "426/426 [==============================] - 0s 156us/sample - loss: 0.2502 - val_loss: 0.1630\n",
      "Epoch 35/600\n",
      "426/426 [==============================] - 0s 161us/sample - loss: 0.2358 - val_loss: 0.1493\n",
      "Epoch 36/600\n",
      "426/426 [==============================] - 0s 168us/sample - loss: 0.2323 - val_loss: 0.1477\n",
      "Epoch 37/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.2116 - val_loss: 0.1434\n",
      "Epoch 38/600\n",
      "426/426 [==============================] - 0s 156us/sample - loss: 0.2415 - val_loss: 0.1365\n",
      "Epoch 39/600\n",
      "426/426 [==============================] - 0s 153us/sample - loss: 0.2252 - val_loss: 0.1323\n",
      "Epoch 40/600\n",
      "426/426 [==============================] - 0s 148us/sample - loss: 0.2206 - val_loss: 0.1260\n",
      "Epoch 41/600\n",
      "426/426 [==============================] - 0s 174us/sample - loss: 0.2116 - val_loss: 0.1280\n",
      "Epoch 42/600\n",
      "426/426 [==============================] - 0s 238us/sample - loss: 0.2222 - val_loss: 0.1228\n",
      "Epoch 43/600\n",
      "426/426 [==============================] - 0s 357us/sample - loss: 0.2203 - val_loss: 0.1268\n",
      "Epoch 44/600\n",
      "426/426 [==============================] - 0s 342us/sample - loss: 0.2172 - val_loss: 0.1173\n",
      "Epoch 45/600\n",
      "426/426 [==============================] - 0s 151us/sample - loss: 0.2251 - val_loss: 0.1200\n",
      "Epoch 46/600\n",
      "426/426 [==============================] - 0s 154us/sample - loss: 0.1908 - val_loss: 0.1198\n",
      "Epoch 47/600\n",
      "426/426 [==============================] - 0s 154us/sample - loss: 0.2066 - val_loss: 0.1146\n",
      "Epoch 48/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.2263 - val_loss: 0.1122\n",
      "Epoch 49/600\n",
      "426/426 [==============================] - 0s 148us/sample - loss: 0.1770 - val_loss: 0.1121\n",
      "Epoch 50/600\n",
      "426/426 [==============================] - 0s 151us/sample - loss: 0.1898 - val_loss: 0.1172\n",
      "Epoch 51/600\n",
      "426/426 [==============================] - 0s 150us/sample - loss: 0.1808 - val_loss: 0.1109\n",
      "Epoch 52/600\n",
      "426/426 [==============================] - 0s 156us/sample - loss: 0.1985 - val_loss: 0.1103\n",
      "Epoch 53/600\n",
      "426/426 [==============================] - 0s 156us/sample - loss: 0.1906 - val_loss: 0.1123\n",
      "Epoch 54/600\n",
      "426/426 [==============================] - 0s 154us/sample - loss: 0.1840 - val_loss: 0.1079\n",
      "Epoch 55/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.1713 - val_loss: 0.1041\n",
      "Epoch 56/600\n",
      "426/426 [==============================] - 0s 150us/sample - loss: 0.1724 - val_loss: 0.1026\n",
      "Epoch 57/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.1753 - val_loss: 0.1026\n",
      "Epoch 58/600\n",
      "426/426 [==============================] - 0s 179us/sample - loss: 0.1618 - val_loss: 0.1036\n",
      "Epoch 59/600\n",
      "426/426 [==============================] - 0s 163us/sample - loss: 0.1517 - val_loss: 0.0950\n",
      "Epoch 60/600\n",
      "426/426 [==============================] - 0s 151us/sample - loss: 0.1829 - val_loss: 0.0946\n",
      "Epoch 61/600\n",
      "426/426 [==============================] - 0s 278us/sample - loss: 0.1694 - val_loss: 0.1044\n",
      "Epoch 62/600\n",
      "426/426 [==============================] - 0s 383us/sample - loss: 0.1745 - val_loss: 0.0965\n",
      "Epoch 63/600\n",
      "426/426 [==============================] - 0s 183us/sample - loss: 0.1332 - val_loss: 0.1004\n",
      "Epoch 64/600\n",
      "426/426 [==============================] - 0s 174us/sample - loss: 0.1501 - val_loss: 0.0983\n",
      "Epoch 65/600\n",
      "426/426 [==============================] - 0s 219us/sample - loss: 0.1573 - val_loss: 0.0905\n",
      "Epoch 66/600\n",
      "426/426 [==============================] - 0s 328us/sample - loss: 0.1579 - val_loss: 0.0947\n",
      "Epoch 67/600\n",
      "426/426 [==============================] - 0s 233us/sample - loss: 0.1336 - val_loss: 0.0990\n",
      "Epoch 68/600\n",
      "426/426 [==============================] - 0s 332us/sample - loss: 0.1328 - val_loss: 0.0861\n",
      "Epoch 69/600\n",
      "426/426 [==============================] - 0s 260us/sample - loss: 0.1224 - val_loss: 0.0920\n",
      "Epoch 70/600\n",
      "426/426 [==============================] - 0s 286us/sample - loss: 0.1712 - val_loss: 0.0853\n",
      "Epoch 71/600\n",
      "426/426 [==============================] - 0s 307us/sample - loss: 0.1490 - val_loss: 0.1221\n",
      "Epoch 72/600\n",
      "426/426 [==============================] - 0s 202us/sample - loss: 0.1584 - val_loss: 0.0905\n",
      "Epoch 73/600\n",
      "426/426 [==============================] - 0s 290us/sample - loss: 0.1259 - val_loss: 0.1018\n",
      "Epoch 74/600\n",
      "426/426 [==============================] - 0s 208us/sample - loss: 0.1370 - val_loss: 0.0920\n",
      "Epoch 75/600\n",
      "426/426 [==============================] - 0s 210us/sample - loss: 0.1395 - val_loss: 0.0888\n",
      "Epoch 76/600\n",
      "426/426 [==============================] - 0s 330us/sample - loss: 0.1493 - val_loss: 0.0904\n",
      "Epoch 77/600\n",
      "426/426 [==============================] - 0s 209us/sample - loss: 0.1372 - val_loss: 0.0895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/600\n",
      "426/426 [==============================] - 0s 310us/sample - loss: 0.1569 - val_loss: 0.0894\n",
      "Epoch 79/600\n",
      "426/426 [==============================] - 0s 259us/sample - loss: 0.1465 - val_loss: 0.0877\n",
      "Epoch 80/600\n",
      "426/426 [==============================] - 0s 183us/sample - loss: 0.1322 - val_loss: 0.0918\n",
      "Epoch 81/600\n",
      "426/426 [==============================] - 0s 296us/sample - loss: 0.1286 - val_loss: 0.0879\n",
      "Epoch 82/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.1318 - val_loss: 0.0962\n",
      "Epoch 83/600\n",
      "426/426 [==============================] - 0s 280us/sample - loss: 0.1445 - val_loss: 0.0894\n",
      "Epoch 84/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.1255 - val_loss: 0.0889\n",
      "Epoch 85/600\n",
      "426/426 [==============================] - 0s 199us/sample - loss: 0.1231 - val_loss: 0.0840\n",
      "Epoch 86/600\n",
      "426/426 [==============================] - 0s 349us/sample - loss: 0.1340 - val_loss: 0.0928\n",
      "Epoch 87/600\n",
      "426/426 [==============================] - 0s 214us/sample - loss: 0.1326 - val_loss: 0.0909\n",
      "Epoch 88/600\n",
      "426/426 [==============================] - 0s 353us/sample - loss: 0.1322 - val_loss: 0.0963\n",
      "Epoch 89/600\n",
      "426/426 [==============================] - 0s 273us/sample - loss: 0.1104 - val_loss: 0.0824\n",
      "Epoch 90/600\n",
      "426/426 [==============================] - 0s 291us/sample - loss: 0.1098 - val_loss: 0.0901\n",
      "Epoch 91/600\n",
      "426/426 [==============================] - 0s 245us/sample - loss: 0.1157 - val_loss: 0.0863\n",
      "Epoch 92/600\n",
      "426/426 [==============================] - 0s 185us/sample - loss: 0.1143 - val_loss: 0.0824\n",
      "Epoch 93/600\n",
      "426/426 [==============================] - 0s 308us/sample - loss: 0.1316 - val_loss: 0.0848\n",
      "Epoch 94/600\n",
      "426/426 [==============================] - 0s 227us/sample - loss: 0.1192 - val_loss: 0.0802\n",
      "Epoch 95/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.1191 - val_loss: 0.0795\n",
      "Epoch 96/600\n",
      "426/426 [==============================] - 0s 165us/sample - loss: 0.1284 - val_loss: 0.0803\n",
      "Epoch 97/600\n",
      "426/426 [==============================] - 0s 148us/sample - loss: 0.1257 - val_loss: 0.0867\n",
      "Epoch 98/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.1387 - val_loss: 0.0808\n",
      "Epoch 99/600\n",
      "426/426 [==============================] - 0s 161us/sample - loss: 0.1139 - val_loss: 0.0930\n",
      "Epoch 100/600\n",
      "426/426 [==============================] - 0s 157us/sample - loss: 0.1184 - val_loss: 0.0792\n",
      "Epoch 101/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.1235 - val_loss: 0.0839\n",
      "Epoch 102/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.1192 - val_loss: 0.0887\n",
      "Epoch 103/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.1141 - val_loss: 0.0804\n",
      "Epoch 104/600\n",
      "426/426 [==============================] - 0s 157us/sample - loss: 0.1398 - val_loss: 0.0845\n",
      "Epoch 105/600\n",
      "426/426 [==============================] - 0s 183us/sample - loss: 0.1211 - val_loss: 0.0820\n",
      "Epoch 106/600\n",
      "426/426 [==============================] - 0s 187us/sample - loss: 0.0988 - val_loss: 0.0834\n",
      "Epoch 107/600\n",
      "426/426 [==============================] - 0s 184us/sample - loss: 0.1179 - val_loss: 0.0896\n",
      "Epoch 108/600\n",
      "426/426 [==============================] - 0s 168us/sample - loss: 0.1050 - val_loss: 0.0909\n",
      "Epoch 109/600\n",
      "426/426 [==============================] - 0s 407us/sample - loss: 0.0944 - val_loss: 0.0804\n",
      "Epoch 110/600\n",
      "426/426 [==============================] - 0s 303us/sample - loss: 0.1021 - val_loss: 0.0832\n",
      "Epoch 111/600\n",
      "426/426 [==============================] - 0s 165us/sample - loss: 0.0996 - val_loss: 0.0825\n",
      "Epoch 112/600\n",
      "426/426 [==============================] - 0s 174us/sample - loss: 0.0938 - val_loss: 0.0799\n",
      "Epoch 113/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0991 - val_loss: 0.0786\n",
      "Epoch 114/600\n",
      "426/426 [==============================] - 0s 156us/sample - loss: 0.1012 - val_loss: 0.0816\n",
      "Epoch 115/600\n",
      "426/426 [==============================] - 0s 144us/sample - loss: 0.1274 - val_loss: 0.0835\n",
      "Epoch 116/600\n",
      "426/426 [==============================] - 0s 161us/sample - loss: 0.1014 - val_loss: 0.0765\n",
      "Epoch 117/600\n",
      "426/426 [==============================] - 0s 158us/sample - loss: 0.1189 - val_loss: 0.0817\n",
      "Epoch 118/600\n",
      "426/426 [==============================] - 0s 156us/sample - loss: 0.0996 - val_loss: 0.0896\n",
      "Epoch 119/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.1175 - val_loss: 0.0980\n",
      "Epoch 120/600\n",
      "426/426 [==============================] - 0s 162us/sample - loss: 0.1035 - val_loss: 0.0908\n",
      "Epoch 121/600\n",
      "426/426 [==============================] - 0s 157us/sample - loss: 0.1199 - val_loss: 0.0935\n",
      "Epoch 122/600\n",
      "426/426 [==============================] - 0s 174us/sample - loss: 0.1165 - val_loss: 0.0864\n",
      "Epoch 123/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.1070 - val_loss: 0.0859\n",
      "Epoch 124/600\n",
      "426/426 [==============================] - 0s 298us/sample - loss: 0.0898 - val_loss: 0.0835\n",
      "Epoch 125/600\n",
      "426/426 [==============================] - 0s 205us/sample - loss: 0.0989 - val_loss: 0.0793\n",
      "Epoch 126/600\n",
      "426/426 [==============================] - 0s 153us/sample - loss: 0.0941 - val_loss: 0.0893\n",
      "Epoch 127/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.0993 - val_loss: 0.0872\n",
      "Epoch 128/600\n",
      "426/426 [==============================] - 0s 151us/sample - loss: 0.0981 - val_loss: 0.0778\n",
      "Epoch 129/600\n",
      "426/426 [==============================] - 0s 158us/sample - loss: 0.0855 - val_loss: 0.0850\n",
      "Epoch 130/600\n",
      "426/426 [==============================] - 0s 144us/sample - loss: 0.0978 - val_loss: 0.0806\n",
      "Epoch 131/600\n",
      "426/426 [==============================] - 0s 156us/sample - loss: 0.1040 - val_loss: 0.0826\n",
      "Epoch 132/600\n",
      "426/426 [==============================] - 0s 151us/sample - loss: 0.0835 - val_loss: 0.0861\n",
      "Epoch 133/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0988 - val_loss: 0.0832\n",
      "Epoch 134/600\n",
      "426/426 [==============================] - 0s 157us/sample - loss: 0.0878 - val_loss: 0.0805\n",
      "Epoch 135/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0932 - val_loss: 0.0817\n",
      "Epoch 136/600\n",
      "426/426 [==============================] - 0s 150us/sample - loss: 0.1071 - val_loss: 0.0769\n",
      "Epoch 137/600\n",
      "426/426 [==============================] - 0s 145us/sample - loss: 0.0874 - val_loss: 0.0788\n",
      "Epoch 138/600\n",
      "426/426 [==============================] - 0s 144us/sample - loss: 0.0951 - val_loss: 0.0754\n",
      "Epoch 139/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0776 - val_loss: 0.0837\n",
      "Epoch 140/600\n",
      "426/426 [==============================] - 0s 165us/sample - loss: 0.0920 - val_loss: 0.0807\n",
      "Epoch 141/600\n",
      "426/426 [==============================] - 0s 167us/sample - loss: 0.0855 - val_loss: 0.0910\n",
      "Epoch 142/600\n",
      "426/426 [==============================] - 0s 173us/sample - loss: 0.0960 - val_loss: 0.0926\n",
      "Epoch 143/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0909 - val_loss: 0.0958\n",
      "Epoch 144/600\n",
      "426/426 [==============================] - 0s 167us/sample - loss: 0.0952 - val_loss: 0.0819\n",
      "Epoch 145/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.1077 - val_loss: 0.0809\n",
      "Epoch 146/600\n",
      "426/426 [==============================] - 0s 156us/sample - loss: 0.0906 - val_loss: 0.0922\n",
      "Epoch 147/600\n",
      "426/426 [==============================] - 0s 142us/sample - loss: 0.0742 - val_loss: 0.0770\n",
      "Epoch 148/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0753 - val_loss: 0.0743\n",
      "Epoch 149/600\n",
      "426/426 [==============================] - 0s 152us/sample - loss: 0.0891 - val_loss: 0.0862\n",
      "Epoch 150/600\n",
      "426/426 [==============================] - 0s 163us/sample - loss: 0.0814 - val_loss: 0.0888\n",
      "Epoch 151/600\n",
      "426/426 [==============================] - 0s 151us/sample - loss: 0.1043 - val_loss: 0.0851\n",
      "Epoch 152/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.0757 - val_loss: 0.0955\n",
      "Epoch 153/600\n",
      "426/426 [==============================] - 0s 158us/sample - loss: 0.0893 - val_loss: 0.0822\n",
      "Epoch 154/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.0751 - val_loss: 0.0745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.0884 - val_loss: 0.0842\n",
      "Epoch 156/600\n",
      "426/426 [==============================] - 0s 158us/sample - loss: 0.0827 - val_loss: 0.0893\n",
      "Epoch 157/600\n",
      "426/426 [==============================] - 0s 148us/sample - loss: 0.0763 - val_loss: 0.0812\n",
      "Epoch 158/600\n",
      "426/426 [==============================] - 0s 153us/sample - loss: 0.0777 - val_loss: 0.0838\n",
      "Epoch 159/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.1066 - val_loss: 0.0790\n",
      "Epoch 160/600\n",
      "426/426 [==============================] - 0s 142us/sample - loss: 0.0906 - val_loss: 0.0831\n",
      "Epoch 161/600\n",
      "426/426 [==============================] - 0s 161us/sample - loss: 0.0771 - val_loss: 0.0763\n",
      "Epoch 162/600\n",
      "426/426 [==============================] - 0s 154us/sample - loss: 0.0802 - val_loss: 0.0799\n",
      "Epoch 163/600\n",
      "426/426 [==============================] - 0s 165us/sample - loss: 0.0911 - val_loss: 0.0867\n",
      "Epoch 164/600\n",
      "426/426 [==============================] - 0s 153us/sample - loss: 0.0858 - val_loss: 0.1019\n",
      "Epoch 165/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0700 - val_loss: 0.0827\n",
      "Epoch 166/600\n",
      "426/426 [==============================] - 0s 196us/sample - loss: 0.0888 - val_loss: 0.0821\n",
      "Epoch 167/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0675 - val_loss: 0.0801\n",
      "Epoch 168/600\n",
      "426/426 [==============================] - 0s 155us/sample - loss: 0.0853 - val_loss: 0.0797\n",
      "Epoch 169/600\n",
      "426/426 [==============================] - 0s 165us/sample - loss: 0.0775 - val_loss: 0.0840\n",
      "Epoch 170/600\n",
      "426/426 [==============================] - 0s 174us/sample - loss: 0.0802 - val_loss: 0.0785\n",
      "Epoch 171/600\n",
      "426/426 [==============================] - 0s 172us/sample - loss: 0.0775 - val_loss: 0.0799\n",
      "Epoch 172/600\n",
      "426/426 [==============================] - 0s 203us/sample - loss: 0.0842 - val_loss: 0.0839\n",
      "Epoch 173/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.0820 - val_loss: 0.0768\n",
      "Epoch 00173: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f055823a1d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=600,\n",
    "          validation_data=(X_test, y_test), verbose=1,\n",
    "          callbacks=[early_stop, board]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
